{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import torch.nn as NN\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import SMART_FEDERAL as SF\n",
    "from distillation import *\n",
    "from train import train\n",
    "from test import test\n",
    "import time\n",
    "from model import CNN_Model\n",
    "import os\n",
    "import csv\n",
    "from utils import XKs, Measure, get_result\n",
    "from dataloader import MyDataset\n",
    "\n",
    "\n",
    "def main(world_size, epochs, rank, batch_size=200, backend='nccl', data_path='/dataset',\n",
    "         lr=1e-5, momentum=0.01, no_cuda=False, seed=35, aggregation_method='naive',\n",
    "         load_model = False, load_path = '/data'\n",
    "        ):\n",
    "    '''Main Function'''\n",
    "\n",
    "#     parser = argparse.ArgumentParser(description='PyTorch: Deep Mutual Learning')\n",
    "#     parser.add_argument('--worker_size', type=int, default=5, metavar='N',\n",
    "#                         help='the number of wokers/nodes (default: 5)')\n",
    "#     parser.add_argument('--batch_size', type=int, default=128, metavar='N',\n",
    "#                         help='input batch size for training (default: 128)')\n",
    "#     parser.add_argument('--test_batch_size', type=int, default=1000, metavar='N',\n",
    "#                         help='input batch size for testing (default: 1000)')\n",
    "#     parser.add_argument('--epochs', type=int, default=50000, metavar='N',\n",
    "#                         help='number of epochs to train (default: 50000)')\n",
    "#     parser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n",
    "#                         help='learning rate (default: 0.01)')\n",
    "#     parser.add_argument('--momentum', type=float, default=0.5, metavar='M',\n",
    "#                         help='SGD momentum (default: 0.5)')\n",
    "#     parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "#                         help='disables CUDA training')\n",
    "#     parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "#                         help='random seed (default: 1)')\n",
    "#     parser.add_argument('--aggregation_method_pool', type=str, default='naive',\n",
    "#                         help='choices of aggregation method')\n",
    "#     parser.add_argument('--save-model', action='store_true', default=True,\n",
    "#                         help='For Saving the current Model')\n",
    "#     parser.add_argument('--rank', type=int,\n",
    "#                         help='the id of node. 0 is master and others are servants')\n",
    "#     parser.add_argument('--backend', type=str, default='nccl',\n",
    "#                         help='communication protocol')\n",
    "#     parser.add_argument('--data_path', type=str, default='/dataset',\n",
    "#                         help='data path')\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "    use_cuda = not no_cuda and torch.cuda.is_available() #使用的使用多个gpu进行训练，需要改进一下\n",
    "    torch.manual_seed(seed)\n",
    "    timeline = time.strftime('%m%d%Y_%H:%M', time.localtime())\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    aggregation_method_pool = [\"naive\", \"bsz_average\", \"weight_average\", \"distillation\"]\n",
    "    ratio = 0.8551957853612336\n",
    "\n",
    "    weight = torch.FloatTensor([ratio, 1 - ratio]).to(device)\n",
    "    Loss = NN.BCELoss(weight=weight)\n",
    "\n",
    "    if rank == 0:\n",
    "        name = 'master'\n",
    "        result_dir = 'result_' + timeline + '/' + name + '/'\n",
    "        model_dir = 'models_' + timeline + '/' + name + '/'\n",
    "        csvname = '{}_log'.format(name) + timeline + '.csv'\n",
    "        modelname = 'model_{:d}.pth'\n",
    "                             \n",
    "        if aggregation_method == 'distillation':\n",
    "            DistillationData = MyDataset(root=data_path, train=True, data_root='distillation.csv')\n",
    "            distillation_dataloader = DataLoader(dataset=DistillationData, batch_size=batch_size,\n",
    "                                                 shuffle=True, drop_last=True)\n",
    "        \n",
    "        model_set = []\n",
    "        for worker_id in range(world_size):\n",
    "            model_set.append(CNN_Model().to(device))\n",
    "             \n",
    "        if load_model:\n",
    "            if aggregation_method == 'distillation':\n",
    "                raise ValueError('Unexpected model')\n",
    "            for worker_id in range(world_size):\n",
    "                model_set[worker_id].load_state_dict(torch.load(load_path))\n",
    "            \n",
    "        opt_set = []\n",
    "        for worker_id in range(world_size):\n",
    "            opt_set.append(optim.SGD(model_set[worker_id].parameters(), lr=lr, momentum=momentum))\n",
    "\n",
    "        model = SF.Master(model=model_set[0], backend=backend, rank=rank, world_size=world_size, learning_rate=lr,\n",
    "                          device=device, aggregation_method=aggregation_method)\n",
    "        for epoch in range(1, epochs+1):\n",
    "            model.train()\n",
    "            model.step(model_buffer=model_set)\n",
    "            model.update(model_set[1:])\n",
    "            if aggregation_method == 'distillation':         \n",
    "                distillation(NN_set=model_set[1:], opt_set=opt_set[1:], dataset=distillation_dataloader,\n",
    "                             world_size=world_size, epoch=epoch, device=device)                \n",
    "#                 best_idx = choose_best(NN_set=model_set[1:], name=name, dataset=dataloader,world_size=world_size,\n",
    "#                                        epoch=epoch, Loss=Loss, time=timeline)\n",
    "#                 best_state_dict = model_set[best_idx+1].state_dict()\n",
    "#                 model_set[0].load_state_dict(best_state_dict)\n",
    "\n",
    "#这里要回传所有的模型\n",
    "\n",
    "    else:\n",
    "        name = 'worker'+str(rank)\n",
    "        result_dir = 'result_' + timeline + '/' + name + '/'\n",
    "        model_dir = 'models_' + timeline + '/' + name + '/'\n",
    "        csvname = '{}_log'.format(name) + timeline + '.csv'\n",
    "        modelname = 'model_{:d}.pth'       \n",
    "        \n",
    "        DataSet_train = MyDataset(root=data_path, train=True, data_root='{}.csv'.format(name)) \n",
    "        dataloader_train = DataLoader(dataset=DataSet_train, batch_size=batch_size, shuffle=True,\n",
    "                                drop_last=True)\n",
    "        DataSet_test = MyDataset(root=data_path, train=True, data_root='{}.csv'.format('test')) \n",
    "        dataloader_test = DataLoader(dataset=DataSet_test, batch_size=batch_size, shuffle=True,\n",
    "                                drop_last=True)\n",
    "        \n",
    "        model_set = []\n",
    "        for worker_id in range(world_size):\n",
    "            model_set.append(CNN_Model().to(device))\n",
    "             \n",
    "        if load_model:\n",
    "            for worker_id in range(world_size):\n",
    "                model_set[worker_id].load_state_dict(torch.load(load_path))\n",
    "        backup_model = CNN_Model().to(device)\n",
    "        train_model = model_set[0]\n",
    "        optimizer = optim.SGD(train_model.parameters(), lr=lr, momentum=momentum)  \n",
    "        model = SF.Servent(model=train_model, backend=backend, rank=rank, world_size=world_size,\n",
    "                           device=device, aggregation_method=aggregation_method)\n",
    "        for epoch in range(1, epochs+1):\n",
    "            model.train()\n",
    "            model.step(model_buffer=model_set, rank=rank)\n",
    "\n",
    "            best_state_dict = train_model.state_dict()\n",
    "            backup_model.load_state_dict(best_state_dict)\n",
    "\n",
    "            train(dataloader=dataloader_train, model=train_model, optimizer=optimizer, Loss = Loss, \n",
    "                  epoch=epoch, time=timeline, result_dir=result_dir, model_dir=model_dir, device=device,\n",
    "                  csvname=csvname, modelname=modelname)\n",
    "\n",
    "            model.update(backup_model)\n",
    "\n",
    "            test(dataloader=dataloader_test, model=train_model, epoch=epoch, Loss=Loss, time=timeline,\n",
    "                 result_dir=result_dir, model_dir=model_dir, csvname=csvname, modelname=modelname,\n",
    "                 device=device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def init_process():\n",
    "#     os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "#     os.environ[\"MASTER_PORT\"] = \"3456\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(world_size=4, epochs=2000, rank=0, batch_size=200, \n",
    "     backend='nccl', data_path='./sp_data/',\n",
    "     lr=1e-5, momentum=0.01, no_cuda=False, seed=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
